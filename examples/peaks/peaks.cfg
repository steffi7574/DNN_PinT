################################
# Data set 
################################

# relative data folder location 
datafolder = ./examples/peaks/
# filename of training data feature vectors
ftrain_ex = features_training.dat
# filename of training data labels/classes
ftrain_labels = labels_training.dat
# filename of validation data feature 
fval_ex = features_validation.dat
# filename of validation data labels/classes 
fval_labels = labels_validation.dat
# number of training data elements (that many lines will be read!) 
ntraining = 5000
# number of validation data elements (that many lines will be read!)
nvalidation = 200
# number of features within the training and validation data set
nfeatures = 2
# number of labels/classes within the training and validation data set
nclasses = 5

# filename for opening weights and bias (set to NONE if not given)
weightsopenfile = NONE
# filename for classification weights and bias (set to NONE if not given)
weightsclassificationfile = NONE

################################
# Neural Network  
################################

# number of channels
nchannels = 8
# number of layers (including opening layer and classification layer) (nlayer >= 3 !)
nlayers = 32    
# final time
T = 1.0
# Activation function ("tanh" or "ReLu" or "SmoothReLu")
activation = SmoothReLu
# Type of network ("dense" the default, or "convolutional")
network_type = dense 
# Opening layer type.  
#  "replicate": replicate image for each convolution.  
#  "activate": same as replicate, only apply tuned, shifted tanh activation function for MNIST. 
type_openlayer = activate
# factor for scaling initial opening layer weights and bias
weights_open_init = 1e-3
# factor for scaling initial weights and bias of intermediate layers
weights_init = 1e-3
# factor for scaling initial classification weights and bias 
weights_class_init = 1e-3

################################
# Nested Iteration 
################################

# Number of nested iteration levels to use
NI_levels = 2
# Amount to refine the network each nested iteration
NI_rfactor = 2
# Interpolation type for new layers: 0 (piece-wise constant), 1 (linear) 
NI_interp_type = 1
# Per level NI training tolerances (when to stop training and move to next finer level)
#   Note, you cannot put spaces between loss tolerances
#   NI_tols[0]: Tolerance used on the coarsest, smallest level
NI_tols = 50,50

################################
# XBraid 
################################

# coarsening factor on level 0
#   generally, cfactor0 = nlayers / P_t
#   where P_t is the processors in time, and nlayers is the number of time-steps
braid_cfactor0 = 2 
# coarsening factor on all other levels
braid_cfactor = 2 
# maximum number of levels 
braid_maxlevels = 10
# minimum allowed coarse time time grid size (values in 10-30 are usually best)
braid_mincoarse = 10
# maximum number of iterations
braid_maxiter = 2
# absolute tolerance
braid_abstol = 1e-15
# absolute adjoint tolerance
braid_adjtol = 1e-15
# printlevel
braid_printlevel = 1
# access level
braid_accesslevel = 0 
# skip work on downcycle?
braid_setskip = 0 
# V-cycle (0) or full multigrid  (1)
braid_fmg = 0
# Number of CF relaxations
braid_nrelax = 1
# Number of CF relaxations on level 0  (1 or 0 are usually the best values)
braid_nrelax0 = 0

####################################
# Optimization
####################################
# Type of batch selection ("deterministic" or "stochastic")
# deterministic:
# fixes batch size => trains on this one
#
# stochastic: uses some (dead) pool
# batch elements are randomly chosen in each iteration during training
# smaller batch size makes sense
batch_type = deterministic
# Batch size
nbatch = 5000
# relaxation param for tikhonov term
gamma_tik = 1e-7
# relaxation param for time-derivative term
gamma_ddt = 1e-5
# relaxation param for tikhonov term of classification weights 
gamma_class = 1e-7
# stepsize selection type ("fixed" or "backtrackingLS" or "oneoverk")
# determines how to choose alpha in design update x_new = x_old - alpha * direction
# fixed          : constant alpha being the initial stepsize
# backtrackingLS : find alpha from backtracking linesearch, starting at initial stepsize
# oneoverk       : alpha = 1/k  where k is the current optimization iteration index
stepsize_type = backtrackingLS
# initial stepsize
stepsize = 1.0
# maximum number of optimization iterations
optim_maxiter = 2
# absolute stopping criterion for the gradient norm
gtol = 1e-4
# maximum number of linesearch iterations
ls_maxiter = 15
# factor for modifying the stepsize within a linesearch iteration
ls_factor = 0.5
# Hessian Approximation ("BFGS", "L-BFGS" or "Identity")
hessian_approx = L-BFGS
# number of stages for l-bfgs method 
lbfgs_stages = 10
# level for validation computation: 
#  -1 = never validate
#   0 = validate only after optimization finishes. 
#   1 = validate in each optimization iteration
validationlevel = 1
